{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução ao Método Preditivo KNN (K-Nearest Neighbors)\n",
    "\n",
    "### Visão Geral do KNN\n",
    "- O KNN é um algoritmo de aprendizado de máquina supervisionado utilizado para classificação e regressão.\n",
    "- Ele é considerado um dos algoritmos mais simples e eficazes para resolver problemas de classificação e regressão.\n",
    "\n",
    "### Princípio Básico\n",
    "- O KNN opera com base na premissa de que objetos semelhantes tendem a estar próximos uns dos outros em um espaço multidimensional.\n",
    "- A ideia central é encontrar os \"vizinhos mais próximos\" de um ponto de dados desconhecido para fazer previsões.\n",
    "\n",
    "### Parâmetro K\n",
    "- O \"K\" em KNN refere-se ao número de vizinhos mais próximos que serão considerados ao fazer uma previsão.\n",
    "- O valor de K é um hiperparâmetro que precisa ser ajustado de acordo com o problema em questão.\n",
    "- Valores pequenos de K podem levar a previsões instáveis e sensíveis a outliers, enquanto valores grandes de K podem suavizar demais a decisão.\n",
    "\n",
    "### Métrica de Distância\n",
    "- Para determinar a proximidade entre pontos de dados, é necessário escolher uma métrica de distância, como a distância euclidiana, a distância de Manhattan ou outras métricas personalizadas.\n",
    "- A escolha da métrica de distância depende da natureza dos dados e do problema em questão.\n",
    "\n",
    "### Processo de Classificação\n",
    "- No caso da classificação, o KNN calcula a classe mais frequente entre os K vizinhos mais próximos e atribui essa classe ao ponto de dados desconhecido.\n",
    "- O KNN pode usar votação ponderada, onde vizinhos mais próximos têm maior influência.\n",
    "\n",
    "### Processo de Regressão\n",
    "- Na regressão, o KNN calcula a média (ou outra medida estatística) dos valores alvo dos K vizinhos mais próximos e atribui esse valor ao ponto de dados desconhecido.\n",
    "\n",
    "### Preparação de Dados\n",
    "- É crucial realizar a normalização ou padronização dos dados antes de aplicar o KNN, já que as métricas de distância são sensíveis à escala dos recursos.\n",
    "\n",
    "### Desafios e Considerações\n",
    "- O KNN pode ser computacionalmente caro para grandes conjuntos de dados, uma vez que requer o cálculo de distâncias para todos os pontos de treinamento.\n",
    "- A seleção adequada de K e da métrica de distância é crítica para o desempenho do algoritmo.\n",
    "- Lidar com dados desbalanceados e outliers também é um desafio importante.\n",
    "\n",
    "### Vantagens e Desvantagens\n",
    "- Vantagens: Simplicidade, eficácia em problemas não lineares, fácil interpretação.\n",
    "- Desvantagens: Sensibilidade a outliers, requer escolha apropriada de K e métrica de distância, baixo desempenho em conjuntos de dados de alta dimensionalidade.\n",
    "\n",
    "### Aplicações\n",
    "- O KNN é utilizado em uma variedade de aplicações, incluindo reconhecimento de padrões, filtragem colaborativa, diagnóstico médico, detecção de fraudes e muito mais.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Gerar dados sintéticos com duas classes\n",
    "X, y = make_blobs(n_samples=100, centers=2, random_state=42)\n",
    "\n",
    "# Inicializar o modelo kNN com k=1\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "# Treinar o modelo com todos os dados\n",
    "knn.fit(X, y)\n",
    "\n",
    "# Criar uma grade de pontos para a fronteira de decisão\n",
    "xx, yy = np.meshgrid(np.linspace(X[:, 0].min() - 1, X[:, 0].max() + 1, 100),\n",
    "                     np.linspace(X[:, 1].min() - 1, X[:, 1].max() + 1, 100))\n",
    "Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plotar os dados e a fronteira de decisão\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.contourf(xx, yy, Z, cmap='coolwarm', alpha=0.6)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', s=50, edgecolors='k', marker='o')\n",
    "plt.title('Fronteira de Decisão do kNN (k=1)')\n",
    "plt.xlabel('Característica 1')\n",
    "plt.ylabel('Característica 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar dados sintéticos com três classes e maior dispersão\n",
    "X, y = make_blobs(n_samples=300, centers=3, cluster_std=1.0, random_state=42)\n",
    "\n",
    "# Inicializar o modelo kNN com k=3\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Treinar o modelo com todos os dados\n",
    "knn.fit(X, y)\n",
    "\n",
    "# Criar uma grade de pontos para a fronteira de decisão\n",
    "xx, yy = np.meshgrid(np.linspace(X[:, 0].min() - 1, X[:, 0].max() + 1, 100),\n",
    "                     np.linspace(X[:, 1].min() - 1, X[:, 1].max() + 1, 100))\n",
    "Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plotar os dados e a fronteira de decisão\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.contourf(xx, yy, Z, cmap='coolwarm', alpha=0.6)\n",
    "scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', s=50, edgecolors='k', marker='o')\n",
    "plt.title('Fronteira de Decisão do kNN (k=3)')\n",
    "plt.xlabel('Característica 1')\n",
    "plt.ylabel('Característica 2')\n",
    "\n",
    "# Plotar os centróides das classes com bolinhas coloridas\n",
    "centroids = knn._tree.query(X, k=3)[1]\n",
    "for i in range(3):\n",
    "    centroid_x = X[centroids[:, i], 0].mean()\n",
    "    centroid_y = X[centroids[:, i], 1].mean()\n",
    "    plt.scatter(centroid_x, centroid_y, c=[i], cmap='coolwarm', s=100, edgecolors='k', marker='o', label=f'Centróide Classe {i + 1}')\n",
    "\n",
    "# Adicionar legenda\n",
    "legend_labels = [f'Classe {i}' for i in range(3)]\n",
    "plt.legend(legend_labels, loc='upper right')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Explicação Teórica\n",
    "\n",
    "\n",
    "### O que é kNN e por que é chamado de método baseado em distância?\n",
    "\n",
    "O KNN, ou K-Nearest Neighbors, é um algoritmo de aprendizado de máquina supervisionado amplamente utilizado em tarefas de classificação e regressão. A principal intuição por trás do KNN é que objetos semelhantes tendem a estar próximos uns dos outros em um espaço multidimensional. Ele é chamado de método baseado em distância porque a sua lógica central gira em torno do cálculo das distâncias entre pontos de dados.\n",
    "\n",
    "### Como o KNN funciona?\n",
    "\n",
    "O funcionamento do KNN pode ser resumido em dois passos principais:\n",
    "\n",
    "#### Cálculo de Distâncias\n",
    "\n",
    "1. **Cálculo de Distâncias**: O primeiro passo envolve o cálculo das distâncias entre o ponto de dados desconhecido (o que você deseja classificar ou prever) e todos os outros pontos de dados no conjunto de treinamento. As distâncias podem ser calculadas utilizando métricas como a distância euclidiana, a distância de Manhattan ou outras métricas personalizadas, dependendo do problema em questão.\n",
    "\n",
    "#### Votação dos k Vizinhos Mais Próximos\n",
    "\n",
    "2. **Votação dos k Vizinhos Mais Próximos**: Após calcular as distâncias, o KNN seleciona os k vizinhos mais próximos do ponto de dados desconhecido. O valor de k é um hiperparâmetro que você precisa definir antes de aplicar o algoritmo. O KNN então realiza uma votação para determinar a classe (no caso de classificação) ou o valor médio (no caso de regressão) do ponto de dados desconhecido com base nas classes ou valores dos k vizinhos mais próximos. A classe ou valor mais frequente se torna a previsão final.\n",
    "\n",
    "Este é o cerne do funcionamento do KNN, onde a similaridade é medida com base na distância entre pontos e a decisão é tomada com base na maioria dos vizinhos mais próximos.\n",
    "\n",
    "Agora que temos uma compreensão básica do KNN e seu funcionamento, avançaremos para as considerações práticas e implementação deste algoritmo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vantagens e Desvantagens do kNN\n",
    "\n",
    "## Vantagens:\n",
    "\n",
    "1. **Simplicidade**: O kNN é um algoritmo simples e fácil de entender. Ele não requer uma suposição explícita sobre a distribuição dos dados e pode ser usado em problemas de classificação e regressão.\n",
    "\n",
    "2. **Adaptabilidade**: O kNN é um algoritmo não paramétrico, o que significa que ele não faz suposições específicas sobre a forma da função que relaciona as entradas às saídas. Isso o torna adaptável a uma variedade de problemas, incluindo aqueles em que a relação entre os recursos e as saídas não é linear.\n",
    "\n",
    "3. **Facilidade de interpretação**: As previsões do kNN podem ser facilmente interpretadas. Podemos identificar quais são os vizinhos mais próximos que contribuem para a classificação de um ponto de dados específico, o que pode ser útil para análise de casos específicos.\n",
    "\n",
    "## Desvantagens:\n",
    "\n",
    "1. **Custo Computacional Alto para Datasets Grandes**: O kNN precisa calcular a distância entre o ponto de consulta e todos os pontos de dados no conjunto de treinamento para fazer uma previsão. Isso pode ser computacionalmente caro em datasets grandes, especialmente se o número de características for alto.\n",
    "\n",
    "2. **Sensível a Características Irrelevantes**: O kNN considera todas as características igualmente importantes no cálculo da distância. Se o conjunto de dados contiver características irrelevantes ou ruidosas, elas podem afetar negativamente o desempenho do algoritmo.\n",
    "\n",
    "3. **Necessidade de Ajuste de Hiperparâmetros**: O valor de 'k' (número de vizinhos) no kNN é um hiperparâmetro que precisa ser ajustado. Escolher um valor adequado para 'k' pode ser um desafio e requer validação cruzada ou outras técnicas de ajuste de hiperparâmetros.\n",
    "\n",
    "4. **Não Lida Bem com Dados Desbalanceados**: Em conjuntos de dados desbalanceados, onde uma classe tem muitos mais exemplos do que outra, o kNN pode ser enviesado em direção à classe majoritária, resultando em classificações menos precisas para a classe minoritária.\n",
    "\n",
    "O kNN é uma ferramenta poderosa, especialmente em cenários onde a simplicidade e a adaptabilidade são importantes. No entanto, é fundamental entender suas vantagens e desvantagens para decidir quando é apropriado usá-lo e como ajustar seus parâmetros adequadamente para obter o melhor desempenho.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 - Aplicaçao prárica\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regressão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar bibliotecas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar os dados\n",
    "df = pd.read_csv('notebooks_regressao.csv')\n",
    "X = df.drop(columns='valor').values\n",
    "y = df[['valor']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar os dados em conjuntos de treinamento, validação e teste\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importância da Normalização de Dados para o k-Nearest Neighbors (kNN)\n",
    "\n",
    "## Por que Normalizar os Dados\n",
    "\n",
    "A normalização de dados desempenha um papel crucial ao usar o algoritmo k-Nearest Neighbors (kNN) para classificação ou regressão. Aqui estão algumas razões pelas quais a normalização é importante:\n",
    "\n",
    "1. **Cálculo de Distâncias**: O kNN depende fortemente do cálculo de distâncias entre pontos de dados no espaço de atributos. Se os dados não estiverem normalizados, características com escalas maiores podem dominar o cálculo de distância, levando a resultados tendenciosos. A normalização garante que todas as características tenham a mesma influência nas distâncias.\n",
    "\n",
    "2. **Evita Problemas de Escala**: Sem a normalização, características com unidades diferentes (por exemplo, quilogramas e metros) podem criar inconsistências nos resultados do kNN. Normalizar os dados resolve essas inconsistências.\n",
    "\n",
    "3. **Melhora o Desempenho**: Normalizar os dados muitas vezes leva a um melhor desempenho do kNN, resultando em decisões mais precisas e modelos mais robustos.\n",
    "\n",
    "## Min-Max Scaler vs. Standard Scaler para kNN\n",
    "\n",
    "Ao normalizar dados para o kNN, você pode escolher entre diferentes técnicas, como o Min-Max Scaler e o Standard Scaler:\n",
    "\n",
    "### Min-Max Scaler\n",
    "\n",
    "O Min-Max Scaler dimensiona as características para um intervalo específico, geralmente entre 0 e 1. É uma boa escolha quando você deseja manter a interpretabilidade das características na mesma escala e quando não quer que uma característica domine as outras devido a valores discrepantes. O Min-Max Scaler é particularmente útil quando os dados têm distribuições não gaussianas.\n",
    "\n",
    "### Standard Scaler\n",
    "\n",
    "O Standard Scaler (também conhecido como Z-score normalization) dimensiona as características de forma que elas tenham média zero e desvio padrão igual a um. É uma escolha sólida quando você deseja remover a média das características e ajustar a variância em torno de 1. Ele é sensível a valores discrepantes, mas isso pode ser benéfico em alguns casos. O Standard Scaler assume que seus dados têm distribuições gaussianas.\n",
    "\n",
    "### Escolha da Técnica\n",
    "\n",
    "A escolha entre Min-Max Scaler e Standard Scaler para o kNN depende da natureza dos seus dados e dos requisitos do seu problema:\n",
    "\n",
    "- Use o Min-Max Scaler quando desejar manter as características em uma escala específica (por exemplo, entre 0 e 1) e evitar que uma característica domine as outras.\n",
    "- Use o Standard Scaler quando desejar que as características tenham média zero e desvio padrão igual a um, e quando seus dados seguirem uma distribuição gaussiana ou quando os valores discrepantes não forem um problema.\n",
    "\n",
    "É importante lembrar que a normalização deve ser aplicada apenas ao conjunto de treinamento, e os mesmos parâmetros de normalização (por exemplo, média e desvio padrão) devem ser usados para normalizar o conjunto de teste ou novos dados.\n",
    "\n",
    "Em resumo, a normalização de dados é fundamental ao usar o k-Nearest Neighbors (kNN) para garantir resultados justos e precisos. A escolha entre Min-Max Scaler e Standard Scaler depende da natureza dos seus dados e das características do seu problema específico.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribuição Gaussiana (Normal)\n",
    "\n",
    "A distribuição gaussiana, também conhecida como distribuição normal, é uma das distribuições de probabilidade mais fundamentais e amplamente utilizadas na estatística e no campo da ciência de dados. Ela é caracterizada por:\n",
    "\n",
    "1. **Curva de Sino**: A distribuição gaussiana tem uma forma de curva de sino simétrica em torno da média (ou valor esperado), que é o seu ponto de máximo.\n",
    "\n",
    "2. **Parâmetros**: A distribuição gaussiana é definida por dois parâmetros principais: a média (μ) e o desvio padrão (σ). A média determina o ponto central da distribuição, enquanto o desvio padrão controla a dispersão dos dados em torno da média.\n",
    "\n",
    "3. **Simetria**: A distribuição gaussiana é simétrica em relação à sua média, o que significa que metade dos dados está acima da média e a outra metade está abaixo dela.\n",
    "\n",
    "4. **Densidade de Probabilidade**: A densidade de probabilidade da distribuição gaussiana é mais alta perto da média e diminui à medida que você se afasta dela. A curva representa a probabilidade de observar valores específicos em uma distribuição.\n",
    "\n",
    "5. **Teorema Central do Limite**: A distribuição gaussiana desempenha um papel fundamental no Teorema Central do Limite, que afirma que a média de uma grande amostra de variáveis aleatórias independentes e identicamente distribuídas se aproxima de uma distribuição gaussiana à medida que o tamanho da amostra aumenta.\n",
    "\n",
    "A distribuição gaussiana é amplamente usada em estatística inferencial, análise de dados, modelagem estatística e em muitas aplicações da ciência de dados, devido à sua propriedade de modelar naturalmente muitos fenômenos na natureza e na sociedade.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm, expon\n",
    "\n",
    "# Dados para a distribuição gaussiana (normal)\n",
    "mu = 0  # Média\n",
    "sigma = 1  # Desvio padrão\n",
    "gaussian_data = np.random.normal(mu, sigma, 1000)\n",
    "\n",
    "# Dados para a distribuição não gaussiana (exponencial)\n",
    "scale = 1  # Parâmetro de escala\n",
    "exponential_data = np.random.exponential(scale, 1000)\n",
    "\n",
    "# Criação dos gráficos\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Gráfico da distribuição gaussiana\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(gaussian_data, bins=30, density=True, color='blue', alpha=0.7)\n",
    "plt.title('Distribuição Gaussiana (Normal)')\n",
    "plt.xlabel('Valores')\n",
    "plt.ylabel('Densidade de Probabilidade')\n",
    "x = np.linspace(-3, 3, 100)\n",
    "plt.plot(x, norm.pdf(x, mu, sigma), 'r-', lw=2)\n",
    "plt.grid(True)\n",
    "\n",
    "# Gráfico da distribuição não gaussiana (exponencial)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(exponential_data, bins=30, density=True, color='green', alpha=0.7)\n",
    "plt.title('Distribuição Não Gaussiana (Exponencial)')\n",
    "plt.xlabel('Valores')\n",
    "plt.ylabel('Densidade de Probabilidade')\n",
    "x = np.linspace(0, 5, 100)\n",
    "plt.plot(x, expon.pdf(x, scale), 'r-', lw=2)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padronizar os atributos (features)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Por que não normalizar a variável alvo (y) em uma regressão com KNN?\n",
    "\n",
    "1. **Natureza da Variável Alvo**:\n",
    "   - A variável alvo (y) em um problema de regressão geralmente representa o valor que você está tentando prever, como preços, pontuações, ou qualquer outra medida numérica contínua.\n",
    "   - A escala da variável alvo é uma parte intrínseca do problema e representa diretamente a grandeza que você deseja estimar.\n",
    "\n",
    "2. **Métricas de Distância em Atributos (X)**:\n",
    "   - No KNN para regressão, as métricas de distância são calculadas com base nos atributos (X), não na variável alvo (y).\n",
    "   - A normalização de y não afeta o cálculo das distâncias entre pontos de dados com diferentes valores de y.\n",
    "\n",
    "3. **Independência de Escala**:\n",
    "   - O KNN é um algoritmo que é independente da escala dos valores da variável alvo.\n",
    "   - A normalização de y não afeta a similaridade ou a distância entre pontos de dados, que é o aspecto crítico do KNN.\n",
    "\n",
    "4. **Interpretação Direta**:\n",
    "   - A normalização de y pode dificultar a interpretação direta dos resultados do modelo.\n",
    "   - Manter a variável alvo em sua escala original facilita a compreensão dos resultados, pois eles têm significado direto.\n",
    "\n",
    "5. **Diferentes Alvos, Diferentes Escalas**:\n",
    "   - Em problemas de regressão, diferentes conjuntos de dados podem ter variáveis alvo com escalas muito diferentes.\n",
    "   - Normalizar y em um problema pode não ser apropriado em outro, tornando a normalização da variável alvo inconsistente.\n",
    "\n",
    "6. **Concentração nas Features (X)**:\n",
    "   - A normalização geralmente se concentra em padronizar os atributos (X) para garantir que as métricas de distância considerem todos os atributos igualmente.\n",
    "   - O foco está na consistência das escalas dos atributos, não na variável alvo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar o modelo KNN para regressão\n",
    "knn_regressor = KNeighborsRegressor(n_neighbors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar o modelo com o conjunto de treinamento\n",
    "knn_regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazer previsões no conjunto de validação\n",
    "y_val_pred = knn_regressor.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliar o desempenho do modelo\n",
    "mse = mean_squared_error(y_val, y_val_pred)\n",
    "print(f\"Erro Quadrático Médio (MSE) na validação: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Por que Selecionar o Número Ideal de Vizinhos (k) em uma Regressão com KNN?\n",
    "\n",
    "1. **Impacto no Desempenho**:\n",
    "   - A escolha do número de vizinhos (k) no algoritmo KNN tem um impacto significativo no desempenho do modelo de regressão.\n",
    "   - Um valor inadequado de k pode levar a um modelo subajustado (underfitting) ou superajustado (overfitting).\n",
    "\n",
    "2. **Balanceamento entre Viés e Variância**:\n",
    "   - O número de vizinhos (k) influencia o equilíbrio entre viés e variância do modelo.\n",
    "   - Um valor pequeno de k (por exemplo, k = 1) resulta em um modelo de alta variância (susceptível a overfitting), pois é sensível a ruído nos dados.\n",
    "   - Um valor grande de k (por exemplo, k = N, onde N é o tamanho do conjunto de dados) resulta em um modelo de alto viés (susceptível a underfitting), pois simplifica demais as previsões.\n",
    "\n",
    "3. **Busca pela Generalização Ideal**:\n",
    "   - O objetivo é encontrar o valor de k que permite que o modelo generalize bem para dados não vistos, fazendo previsões precisas.\n",
    "   - A seleção cuidadosa de k ajuda a evitar a subestimação ou superestimação dos relacionamentos nos dados.\n",
    "\n",
    "4. **Validação Cruzada como Ferramenta**:\n",
    "   - A seleção de hiperparâmetros, como k, é geralmente realizada usando técnicas de validação cruzada, como a validação cruzada k-fold.\n",
    "   - A validação cruzada ajuda a avaliar o desempenho do modelo com diferentes valores de k e escolher o valor que minimiza o erro de previsão.\n",
    "\n",
    "5. **Evitar Overfitting e Underfitting**:\n",
    "   - Selecionar o número ideal de vizinhos ajuda a evitar problemas de overfitting (quando o modelo é muito complexo) e underfitting (quando o modelo é muito simples).\n",
    "   - Um valor apropriado de k permite que o modelo encontre um equilíbrio entre flexibilidade e estabilidade.\n",
    "\n",
    "6. **Impacto nas Previsões**:\n",
    "   - A escolha do valor de k também afeta diretamente as previsões do modelo. Valores diferentes de k podem resultar em previsões substancialmente diferentes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escolha uma faixa de valores de k que você deseja testar\n",
    "k_values = range(1, 21)\n",
    "\n",
    "# Inicialize uma lista para armazenar os valores de MSE para cada valor de k\n",
    "mse_values = []\n",
    "\n",
    "# Realize a validação cruzada para cada valor de k\n",
    "\n",
    "for k in k_values:\n",
    "    # Crie um modelo KNN Regressor com o valor atual de k\n",
    "    knn_regressor = KNeighborsRegressor(n_neighbors=k)\n",
    "    \n",
    "    # Treine o modelo com os dados de treinamento\n",
    "    knn_regressor.fit(X_train, y_train)\n",
    "    \n",
    "    # Faça previsões no conjunto de teste\n",
    "    y_pred = knn_regressor.predict(X_val)\n",
    "    \n",
    "    # Calcule o Erro Quadrático Médio (MSE)\n",
    "    mse = mean_squared_error(y_val, y_pred)\n",
    "    \n",
    "    # Armazene o valor de MSE na lista\n",
    "    mse_values.append(mse)\n",
    "\n",
    "# Plote a curva do MSE em relação ao número de vizinhos (k)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values, mse_values, marker='o', linestyle='-', color='b')\n",
    "plt.xlabel('Número de Vizinhos (k)')\n",
    "plt.ylabel('Erro Quadrático Médio (MSE)')\n",
    "plt.title('Validação Cruzada para Seleção de k em KNN Regressão')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Encontre o valor de k que minimiza o MSE\n",
    "best_k = k_values[np.argmin(mse_values)]\n",
    "print(f\"O valor ideal de k é {best_k} com MSE mínimo de {min(mse_values)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar o modelo KNN para regressão\n",
    "knn_regressor = KNeighborsRegressor(n_neighbors=10)\n",
    "\n",
    "# Treinar o modelo com o conjunto de treinamento\n",
    "knn_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Fazer previsões no conjunto de validação\n",
    "y_val_pred = knn_regressor.predict(X_val)\n",
    "\n",
    "# Avaliar o desempenho do modelo\n",
    "mse = mean_squared_error(y_val, y_val_pred)\n",
    "print(f\"Erro Quadrático Médio (MSE) na validação: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazer previsões no conjunto de teste\n",
    "y_test_pred = knn_regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliar o desempenho no conjunto de teste\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "print(f\"Erro Quadrático Médio (MSE) no teste: {mse_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotar os resultados, se desejado\n",
    "plt.scatter(y_test[500:550], y_test_pred[500:550])\n",
    "plt.xlabel('Preço Real')\n",
    "plt.ylabel('Preço Previsto')\n",
    "plt.title('Gráfico de Dispersão: Preço Real vs. Preço Previsto')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('notebooks_classificacao.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertendo colunas textuais para numéricas usando One-Hot Encoding\n",
    "for column in df.select_dtypes(include=['object']).columns:\n",
    "    label_encoder = LabelEncoder()\n",
    "    # Ajuste e transforme a coluna categórica\n",
    "    df[column] = label_encoder.fit_transform(df[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns='segmento').values\n",
    "y = df[['segmento']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar os dados em conjuntos de treinamento, validação e teste\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "# Padronizar os atributos (features)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar o classificador KNN com um valor específico de k (número de vizinhos)\n",
    "k = 10\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar o classificador com os dados de treinamento\n",
    "knn_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazer previsões nos dados de teste\n",
    "y_pred = knn_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular a acurácia do modelo\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Acurácia do modelo: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcular a matriz de confusão\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# plotar o heatmap\n",
    "sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\"d\")\n",
    "plt.title('Matriz de Confusão')\n",
    "plt.xlabel('Previsões')\n",
    "plt.ylabel('Real')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbow_method(X_train, y_train, X_val, y_val, max_k=10):\n",
    "    # Inicializa uma lista para armazenar os valores de Erro Quadrático Médio (MSE) para diferentes valores de k\n",
    "    accuracy_values = []\n",
    "    \n",
    "    # Itera através dos valores de k de 1 a max_k\n",
    "    for k in range(1, max_k + 1):\n",
    "        # Cria um modelo KNN Regressor com o valor atual de k\n",
    "        knn_classifier = KNeighborsClassifier(n_neighbors=k)\n",
    "        \n",
    "        # Treina o modelo com os dados de treinamento\n",
    "        knn_classifier.fit(X_train, y_train)\n",
    "        \n",
    "        # Fazer previsões nos dados de teste\n",
    "        y_pred = knn_classifier.predict(X_val)\n",
    "\n",
    "        # Calcula a acurácia entre as previsões e os valores reais\n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "        \n",
    "        # Armazena o valor MSE na lista de valores\n",
    "        accuracy_values.append(accuracy)\n",
    "    \n",
    "    # Plota a curva do cotovelo\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(range(1, max_k + 1), accuracy_values, marker='o', linestyle='-', color='b')\n",
    "    plt.xlabel('Número de Vizinhos (k)')\n",
    "    plt.ylabel('Acurácia')\n",
    "    plt.title('Método do Cotovelo para KNN Classificação')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Método do Cotovelo (Elbow Method) em Aprendizado de Máquina\n",
    "\n",
    "O Método do Cotovelo é uma técnica utilizada para determinar o número ideal de clusters ou vizinhos em algoritmos de aprendizado de máquina, como K-Means ou KNN, respectivamente. Este método ajuda a encontrar um valor apropriado para hiperparâmetros, como 'k', de forma empírica.\n",
    "\n",
    "1. **Definição do Problema**:\n",
    "   - O Método do Cotovelo é útil quando você precisa determinar um valor apropriado para um hiperparâmetro, como o número de clusters (no caso do K-Means) ou o número de vizinhos (no caso do KNN).\n",
    "\n",
    "2. **Variando o Hiperparâmetro**:\n",
    "   - O primeiro passo é variar o valor do hiperparâmetro em questão. Por exemplo, você pode testar valores de 'k' de 1 a um número máximo definido.\n",
    "\n",
    "3. **Treinamento do Modelo**:\n",
    "   - Para cada valor do hiperparâmetro, você treina o modelo correspondente (por exemplo, K-Means ou KNN) com o conjunto de dados de treinamento.\n",
    "\n",
    "4. **Avaliação da Qualidade**:\n",
    "   - Em seguida, você avalia a qualidade do modelo para cada valor do hiperparâmetro usando uma métrica apropriada. Por exemplo, em K-Means, uma métrica comum é a soma dos erros quadráticos (SSE), enquanto no KNN para regressão, o Erro Quadrático Médio (MSE) pode ser usado.\n",
    "\n",
    "5. **Plotagem dos Resultados**:\n",
    "   - Você registra as métricas de avaliação para cada valor do hiperparâmetro e, em seguida, cria um gráfico que mostra a variação das métricas à medida que o hiperparâmetro muda.\n",
    "\n",
    "6. **Identificando o Cotovelo**:\n",
    "   - Ao analisar o gráfico, você procura um ponto de inflexão, que se assemelha a um cotovelo. Este ponto é onde a métrica começa a se estabilizar, e escolher um valor maior para o hiperparâmetro não melhora significativamente o desempenho do modelo.\n",
    "\n",
    "7. **Seleção do Valor Ideal**:\n",
    "   - O valor do hiperparâmetro correspondente ao ponto de cotovelo é considerado o valor ideal para o problema em questão. Por exemplo, o número de clusters ou vizinhos que maximizam o desempenho do modelo.\n",
    "\n",
    "8. **Aplicação Prática**:\n",
    "   - O Método do Cotovelo é amplamente utilizado em tarefas de clustering, seleção de hiperparâmetros e ajuste fino de modelos de aprendizado de máquina para obter um equilíbrio entre subajuste e superajuste.\n",
    "\n",
    "Este método fornece uma abordagem visual e empírica para a seleção de hiperparâmetros, ajudando a escolher valores que resultem em modelos mais eficazes para um determinado problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elbow_method(X_train, y_train, X_val, y_val, max_k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=k)\n",
    "\n",
    "\n",
    "# Treinar o classificador com os dados de treinamento\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Fazer previsões nos dados de teste\n",
    "y_pred = knn_classifier.predict(X_test)\n",
    "\n",
    "\n",
    "# Calcular a acurácia do modelo\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Acurácia do modelo: {accuracy:.2f}')\n",
    "\n",
    "\n",
    "# calcular a matriz de confusão\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# plotar o heatmap\n",
    "sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\"d\")\n",
    "plt.title('Matriz de Confusão')\n",
    "plt.xlabel('Previsões')\n",
    "plt.ylabel('Real')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métricas de Distância"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distâncias para o Algoritmo K-Nearest Neighbors (KNN)\n",
    "\n",
    "O algoritmo K-Nearest Neighbors (KNN) é um método de aprendizado de máquina supervisionado que classifica pontos de dados com base na proximidade com os vizinhos mais próximos. Para calcular essa proximidade, diferentes métricas de distância podem ser usadas. Vamos explorar três delas: a distância Euclidiana, a distância Manhattan e a distância Minkowski.\n",
    "\n",
    "## Distância Euclidiana (Verde)\n",
    "\n",
    "- A distância Euclidiana é a métrica de distância mais comum.\n",
    "- Ela é calculada como a raiz quadrada da soma dos quadrados das diferenças entre as coordenadas de dois pontos.\n",
    "- Fórmula: `d_euclidiana(x, y) = sqrt(Σ(xi - yi)^2)`\n",
    "- Sensível a valores discrepantes.\n",
    "- Apropriada para dados distribuídos de forma contínua.\n",
    "\n",
    "## Distância Manhattan (Azul)\n",
    "\n",
    "- A distância Manhattan, também chamada de distância da cidade, é outra métrica comum.\n",
    "- É calculada como a soma das diferenças absolutas entre as coordenadas de dois pontos.\n",
    "- Fórmula: `d_manhattan(x, y) = Σ|xi - yi|`\n",
    "- Menos sensível a valores discrepantes do que a distância Euclidiana.\n",
    "- Útil quando os dados têm uma estrutura de grade ou características com unidades diferentes.\n",
    "\n",
    "## Distância Minkowski (Amarela p = 2)  (Vermelha p = 1)\n",
    "\n",
    "- A distância Minkowski é uma métrica mais geral que engloba as distâncias Euclidiana e Manhattan.\n",
    "- É definida pela fórmula: `d_minkowski(x, y, p) = (Σ|xi - yi|^p)^(1/p)`\n",
    "- O parâmetro `p` determina o tipo de distância.\n",
    "- Quando `p` é igual a 2, é equivalente à distância Euclidiana.\n",
    "- Quando `p` é igual a 1, é equivalente à distância Manhattan.\n",
    "- Permite ajustar a sensibilidade a discrepâncias nos dados.\n",
    "\n",
    "Essas são as três principais métricas de distância usadas no algoritmo KNN para calcular a proximidade entre pontos de dados. A escolha da métrica apropriada depende da natureza dos dados e dos requisitos do problema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparar_metricas(knn):\n",
    "     # Gerar dados sintéticos com quatro classes mais próximas umas das outras\n",
    "    X, y = make_blobs(n_samples=40, centers=4, cluster_std=3.0, random_state=42)\n",
    "\n",
    "    # Treinar o modelo com todos os dados\n",
    "    knn.fit(X, y)\n",
    "\n",
    "    # Criar uma grade de pontos para a fronteira de decisão\n",
    "    xx, yy = np.meshgrid(np.linspace(X[:, 0].min() - 1, X[:, 0].max() + 1, 100),\n",
    "                        np.linspace(X[:, 1].min() - 1, X[:, 1].max() + 1, 100))\n",
    "    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # Plotar os dados e a fronteira de decisão\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.contourf(xx, yy, Z, cmap='coolwarm', alpha=0.6)\n",
    "    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', s=50, edgecolors='k', marker='o')\n",
    "    plt.title('Fronteira de Decisão do kNN (k=3)')\n",
    "    plt.xlabel('Característica 1')\n",
    "    plt.ylabel('Característica 2')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=3, metric='euclidean')\n",
    "comparar_metricas(knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=3, metric='minkowski', p = 2)\n",
    "comparar_metricas(knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=3, metric='manhattan')\n",
    "comparar_metricas(knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=3, metric='minkowski', p = 1)\n",
    "comparar_metricas(knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=3, metric='minkowski', p = 15)\n",
    "comparar_metricas(knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('notebooks_classificacao.csv')\n",
    "\n",
    "# Convertendo colunas textuais para numéricas usando One-Hot Encoding\n",
    "for column in df.select_dtypes(include=['object']).columns:\n",
    "    label_encoder = LabelEncoder()\n",
    "    # Ajuste e transforme a coluna categórica\n",
    "    df[column] = label_encoder.fit_transform(df[column])\n",
    "\n",
    "X = df.drop(columns='segmento').values\n",
    "y = df[['segmento']]\n",
    "\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "# Padronizar os atributos (features)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Inicializar o classificador KNN com um valor específico de k (número de vizinhos)\n",
    "k = 5\n",
    "\n",
    "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
    "knn_euclidean.fit(X_train, y_train)\n",
    "y_pred_euclidean = knn_euclidean.predict(X_test)\n",
    "accuracy_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
    "\n",
    "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
    "knn_manhattan.fit(X_train, y_train)\n",
    "y_pred_manhattan = knn_manhattan.predict(X_test)\n",
    "accuracy_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
    "\n",
    "knn_minkowski = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p = 5)\n",
    "knn_minkowski.fit(X_train, y_train)\n",
    "y_pred_minkowski = knn_minkowski.predict(X_test)\n",
    "accuracy_minkowski = accuracy_score(y_test, y_pred_minkowski)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Acurária euclidean: {accuracy_euclidean}')\n",
    "print(f'Acurária manhattan: {accuracy_manhattan}')\n",
    "print(f'Acurária minkowski: {accuracy_minkowski}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qual_melhor_p(X_train, y_train, X_val, y_val, max_p=10):\n",
    "    # Inicializa uma lista para armazenar os valores de Erro Quadrático Médio (MSE) para diferentes valores de k\n",
    "    accuracy_values = []\n",
    "    \n",
    "    # Itera através dos valores de k de 1 a max_k\n",
    "    for p in range(1, max_p + 1):\n",
    "        # Cria um modelo KNN Regressor com o valor atual de k\n",
    "        knn_classifier = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p = p)\n",
    "        \n",
    "        # Treina o modelo com os dados de treinamento\n",
    "        knn_classifier.fit(X_train, y_train)\n",
    "        \n",
    "        # Fazer previsões nos dados de teste\n",
    "        y_pred = knn_classifier.predict(X_val)\n",
    "\n",
    "        # Calcula a acurácia entre as previsões e os valores reais\n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "        \n",
    "        # Armazena o valor MSE na lista de valores\n",
    "        accuracy_values.append(accuracy)\n",
    "    \n",
    "    # Plota a curva do cotovelo\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(range(1, max_p + 1), accuracy_values, marker='o', linestyle='-', color='b')\n",
    "    plt.xlabel('Número de p')\n",
    "    plt.ylabel('Acurácia')\n",
    "    plt.title('Verificando qual o melhor número de P')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qual_melhor_p(X_train, y_train, X_val, y_val, max_p=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício\n",
    "\n",
    "\n",
    "### Enunciado:\n",
    "\n",
    "Estamos lidando com um problema de classificação binária. Sua tarefa é construir um modelo que possa determinar se um registro específico pertence a uma das duas classes: se um indivíduo tem câncer de pele ou não. A base de dados possui um total de 10.000 registros, sendo que 5.000 destes são de indivíduos com câncer de pele e os outros 5.000 são de indivíduos sem câncer de pele. A classe alvo é denominada \"SkinCancer\".\n",
    "\n",
    "Siga as etapas abaixo para preparar seus dados e construir o modelo:\n",
    "\n",
    "## Exercício 1:\n",
    "Leia a base de dados nomeada cancer_pele.csv.\n",
    "\n",
    "## Exercício 2:\n",
    "Realize o tratamento dos valores textuais, convertendo-os para formatos numéricos, para que possam ser utilizados em algoritmos de aprendizado de máquina.\n",
    "\n",
    "## Exercício 3:\n",
    "Separe os dados em dois conjuntos: X, que representará as características ou atributos dos registros; e y, que representará a classe alvo \"SkinCancer\".\n",
    "\n",
    "## Exercício 4:\n",
    "Divida os conjuntos X e y em dados de treinamento e teste. 80% dos dados devem ser usados para treinamento e os restantes 20% para teste.\n",
    "\n",
    "## Exercício 5:\n",
    "Normalize os conjuntos X de treinamento e teste para garantir que todos os atributos estejam na mesma escala, o que pode ajudar na convergência e no desempenho dos algoritmos de aprendizado de máquina.\n",
    "\n",
    "## Exercício 6:\n",
    "Utilize o método do cotovelo para determinar a quantidade ideal do valor de k para um algoritmo de agrupamento ou classificação.\n",
    "\n",
    "## Exercício 7:\n",
    "Após definir o valor ideal de k, realize testes para determinar qual métrica de distância oferece os melhores resultados para seu modelo: euclidiana ou manhattan. Compare o desempenho entre elas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
